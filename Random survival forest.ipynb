{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Force matplotlib to not use any Xwindows backend.\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import viz\n",
    "from deepsurv import utils\n",
    "import deepsurv\n",
    "\n",
    "import argparse\n",
    "from collections import defaultdict\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import h5py\n",
    "import uuid\n",
    "from sklearn.utils import resample \n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "# rpy2 lets you run R within Python\n",
    "import rpy2 \n",
    "import rpy2.robjects as robjects\n",
    "from rpy2.robjects.packages import importr\n",
    "# Run a RandomSurvivalForest\n",
    "rfSRC = importr('randomForestSRC')\n",
    "\n",
    "# Converts pandas Dataframes into R Dataframes\n",
    "from rpy2.robjects import pandas2ri\n",
    "pandas2ri.activate()\n",
    "\n",
    "import logging\n",
    "\n",
    "import time\n",
    "localtime   = time.localtime()\n",
    "TIMESTRING  = time.strftime(\"%m%d%Y%M\", localtime)\n",
    "\n",
    "DURATION_COL = 'time'\n",
    "EVENT_COL = 'status'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e\n",
      "t\n",
      "x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\Anaconda3\\lib\\site-packages\\h5py\\_hl\\dataset.py:313: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n",
      "  \"Use dataset[()] instead.\", H5pyDeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "filename = 'C:/Users/ASUS/Dropbox/석사학위논문/DeepSurv-master/data/whas_train_test.h5'\n",
    "f = h5py.File(filename, 'r')\n",
    "\n",
    "#Get the HDF5 group\n",
    "group = f['train']\n",
    "\n",
    "#Checkout what keys are inside that group.\n",
    "for key in group.keys():\n",
    "    print(key)\n",
    "\n",
    "data_x = group['x'].value\n",
    "data_t = group['t'].value\n",
    "data_e = group['e'].value\n",
    "\n",
    "#Do whatever you want with data\n",
    "\n",
    "#After you are done\n",
    "f.close()\n",
    "\n",
    "train_df = pd.DataFrame(data_x)\n",
    "train_df['Time'] = np.int32(data_t)\n",
    "train_df['Event'] = data_e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_df = train_test_split(train_df, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rpy2 lets you run R within Python\n",
    "import rpy2 \n",
    "import rpy2.robjects as robjects\n",
    "from rpy2.robjects.packages import importr\n",
    "# Run a RandomSurvivalForest\n",
    "rfSRC = importr('randomForestSRC')\n",
    "\n",
    "# Converts pandas Dataframes into R Dataframes\n",
    "from rpy2.robjects import pandas2ri\n",
    "pandas2ri.activate()\n",
    "\n",
    "import logging\n",
    "\n",
    "\n",
    "def evaluate_model(model, dataset, bootstrap = False, trt_idx = None):\n",
    "    def ci(model):\n",
    "        def rsf_ci(**kwargs):\n",
    "            data = utils.format_dataset_to_df(kwargs, 't', 'e', trt_idx)\n",
    "            pred_test = rfSRC.predict_rfsrc(model, data)\n",
    "            return 1 - pred_test.rx('err.rate')[0][-1]\n",
    "        return rsf_ci\n",
    "\n",
    "    metrics = {}\n",
    "\n",
    "    # Calculate c_index\n",
    "    metrics['c_index'] = ci(model)(**dataset)\n",
    "    if bootstrap:\n",
    "        metrics['c_index_bootstrap'] = utils.bootstrap_metric(ci(model), dataset)\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WHAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training RSF Model\n",
      "Train/Valid C-Index: 0.832593473741147\n",
      "Test metrics: {'c_index': 0.8348423142943691, 'c_index_bootstrap': {'mean': 0.8372622309877197, 'confidence_interval': (0.8339451292911861, 0.8405793326842532)}}\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    datasets = utils.load_datasets('C:/Users/ASUS/Dropbox/석사학위논문/DeepSurv-master/data/whas_train_test.h5')\n",
    "\n",
    "    # Train CPH model\n",
    "    print(\"Training RSF Model\")\n",
    "    train_df = utils.format_dataset_to_df(datasets['train'], DURATION_COL, EVENT_COL)\n",
    "    surv_f = robjects.r(\"as.formula(Surv(time,status) ~ .)\")\n",
    "    rfsc = rfSRC.rfsrc(surv_f, train_df, ntree=100)\n",
    "\n",
    "    print('Train/Valid C-Index:', 1 - rfsc.rx('err.rate')[0][-1])\n",
    "    if 'valid' in datasets:\n",
    "        metrics = evaluate_model(rfsc, datasets['valid'])\n",
    "        print(\"Valid metrics: \" + str(metrics))\n",
    "\n",
    "    if 'test' in datasets:\n",
    "        metrics = evaluate_model(rfsc, datasets['test'], bootstrap=True)\n",
    "        print(\"Test metrics: \" + str(metrics))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# METABRIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training RSF Model\n",
      "Train/Valid C-Index: 0.6531827893703416\n",
      "Test metrics: {'c_index': 0.6387032707863541, 'c_index_bootstrap': {'mean': 0.6401059391159007, 'confidence_interval': (0.6363905541443751, 0.6438213240874264)}}\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    datasets = utils.load_datasets('C:/Users/ASUS/Dropbox/석사학위논문/DeepSurv-master/data/metabric_IHC4_clinical_train_test.h5')\n",
    "\n",
    "    # Train CPH model\n",
    "    print(\"Training RSF Model\")\n",
    "    train_df = utils.format_dataset_to_df(datasets['train'], DURATION_COL, EVENT_COL)\n",
    "    surv_f = robjects.r(\"as.formula(Surv(time,status) ~ .)\")\n",
    "    rfsc = rfSRC.rfsrc(surv_f, train_df, ntree=100)\n",
    "\n",
    "    print('Train/Valid C-Index:', 1 - rfsc.rx('err.rate')[0][-1])\n",
    "    if 'valid' in datasets:\n",
    "        metrics = evaluate_model(rfsc, datasets['valid'])\n",
    "        print(\"Valid metrics: \" + str(metrics))\n",
    "\n",
    "    if 'test' in datasets:\n",
    "        metrics = evaluate_model(rfsc, datasets['test'], bootstrap=True)\n",
    "        print(\"Test metrics: \" + str(metrics))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GBSG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training RSF Model\n",
      "Train/Valid C-Index: 0.660127137003674\n",
      "Test metrics: {'c_index': 0.6677372580839043, 'c_index_bootstrap': {'mean': 0.6685059341777223, 'confidence_interval': (0.6653539405911867, 0.6716579277642579)}}\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    datasets = utils.load_datasets('C:/Users/ASUS/Dropbox/석사학위논문/DeepSurv-master/data/gbsg_cancer_train_test.h5')\n",
    "\n",
    "    # Train CPH model\n",
    "    print(\"Training RSF Model\")\n",
    "    train_df = utils.format_dataset_to_df(datasets['train'], DURATION_COL, EVENT_COL)\n",
    "    surv_f = robjects.r(\"as.formula(Surv(time,status) ~ .)\")\n",
    "    rfsc = rfSRC.rfsrc(surv_f, train_df, ntree=100)\n",
    "\n",
    "    print('Train/Valid C-Index:', 1 - rfsc.rx('err.rate')[0][-1])\n",
    "    if 'valid' in datasets:\n",
    "        metrics = evaluate_model(rfsc, datasets['valid'])\n",
    "        print(\"Valid metrics: \" + str(metrics))\n",
    "\n",
    "    if 'test' in datasets:\n",
    "        metrics = evaluate_model(rfsc, datasets['test'], bootstrap=True)\n",
    "        print(\"Test metrics: \" + str(metrics))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SUPPORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    datasets = utils.load_datasets('C:/Users/ASUS/Dropbox/석사학위논문/DeepSurv-master/data/support_train_test.h5')\n",
    "\n",
    "    # Train CPH model\n",
    "    print(\"Training RSF Model\")\n",
    "    train_df = utils.format_dataset_to_df(datasets['train'], DURATION_COL, EVENT_COL)\n",
    "    surv_f = robjects.r(\"as.formula(Surv(time,status) ~ .)\")\n",
    "    rfsc = rfSRC.rfsrc(surv_f, train_df, ntree=100)\n",
    "\n",
    "    print('Train/Valid C-Index:', 1 - rfsc.rx('err.rate')[0][-1])\n",
    "    if 'valid' in datasets:\n",
    "        metrics = evaluate_model(rfsc, datasets['valid'])\n",
    "        print(\"Valid metrics: \" + str(metrics))\n",
    "\n",
    "    if 'test' in datasets:\n",
    "        metrics = evaluate_model(rfsc, datasets['test'], bootstrap=True)\n",
    "        print(\"Test metrics: \" + str(metrics))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NWTCO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_number :  4028\n",
      "feature_number :  8\n",
      "censored_number :  3457\n",
      "censored_rate :  85.82423038728898\n"
     ]
    }
   ],
   "source": [
    "#nwtco 데이터 불러오기\n",
    "nwtco = pd.read_csv('C:/Users/ASUS/Dropbox/석사학위논문/DeepSurv-master/data/nwtco.csv')\n",
    "train_df = nwtco.drop('seqno', axis = 1)\n",
    "train_df = train_df.rename(columns = {'rel':'Event', 'edrel':'Time'})\n",
    "\n",
    "#censored number\n",
    "c_number = train_df['Event'].value_counts()[0]\n",
    "feat_num = len(list(train_df.columns))\n",
    "total_number = len(train_df['Event'])\n",
    "c_rate = (c_number / total_number) * 100\n",
    "\n",
    "print(\"total_number : \", total_number)\n",
    "print(\"feature_number : \", feat_num)\n",
    "print(\"censored_number : \", c_number)\n",
    "print(\"censored_rate : \" , c_rate)\n",
    "\n",
    "#data split\n",
    "train_df, test_df = train_test_split(train_df, test_size = 0.2)\n",
    "train_df, val_df = train_test_split(train_df, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7273909351448891"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "surv_f = robjects.r(\"as.formula(Surv(Time,Event) ~ .)\")\n",
    "rfsc = rfSRC.rfsrc(surv_f, train_df, ntree=50)\n",
    "pred_test = rfSRC.predict_rfsrc(rfsc, test_df)\n",
    "rsf_ci = 1 - pred_test.rx('err.rate')[0][-1]\n",
    "rsf_ci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95.0 confidence interval 84.3% and 88.0%\n"
     ]
    }
   ],
   "source": [
    "n_iterations = 1000\n",
    "n_size = int(len(train_df) * 0.50)\n",
    "\n",
    "# run bootstrap\n",
    "stats = list()\n",
    "for i in range(n_iterations):\n",
    "    train = resample(train_df, n_samples=n_size)\n",
    "    surv_f = robjects.r(\"as.formula(Surv(Time,Event) ~ .)\")\n",
    "    rfsc = rfSRC.rfsrc(surv_f, train, ntree=50)\n",
    "    pred_test = rfSRC.predict_rfsrc(rfsc, test_df)\n",
    "    score = 1 - pred_test.rx('err.rate')[0][-1]\n",
    "    stats.append(score)\n",
    "    \n",
    "# confidence intervals\n",
    "alpha = 0.95\n",
    "p = ((1.0-alpha)/2.0) * 100\n",
    "lower = max(0.0, np.percentile(stats, p))\n",
    "p = (alpha+((1.0-alpha)/2.0)) * 100\n",
    "upper = min(1.0, np.percentile(stats, p))\n",
    "print('%.1f confidence interval %.1f%% and %.1f%%' % (alpha*100, lower*100, upper*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8619736234329921"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate data\n",
    "import deepsurv\n",
    "\n",
    "def generate_data(treatment_group = False):\n",
    "    np.random.seed(123)\n",
    "    sd = deepsurv.datasets.SimulatedData(5, num_features = 15,\n",
    "        treatment_group = treatment_group)\n",
    "    train_data = sd.generate_data(1000,method = 'gaussian')\n",
    "    valid_data = sd.generate_data(400,method = 'gaussian')\n",
    "    test_data = sd.generate_data(400,method = 'gaussian')\n",
    "\n",
    "    return train_data, valid_data, test_data\n",
    "\n",
    "train_data, valid_data, test_data = generate_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training RSF Model\n",
      "Train/Valid C-Index: 0.5598493770081854\n",
      "Valid metrics: {'c_index': 0.5786100628930818}\n",
      "Test metrics: {'c_index': 0.5718484898289442, 'c_index_bootstrap': {'mean': 0.5732000893330033, 'confidence_interval': (0.570152048617065, 0.5762481300489415)}}\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    # Train CPH model\n",
    "    print(\"Training RSF Model\")\n",
    "    train_df = utils.format_dataset_to_df(train_data, 't', 'e')\n",
    "    surv_f = robjects.r(\"as.formula(Surv(t,e) ~ .)\")\n",
    "    rfsc = rfSRC.rfsrc(surv_f, train_df, ntree=3)\n",
    "\n",
    "    print('Train/Valid C-Index:', 1 - rfsc.rx('err.rate')[0][-1])\n",
    "    metrics = evaluate_model(rfsc, valid_data)\n",
    "    print(\"Valid metrics: \" + str(metrics))\n",
    "\n",
    "    metrics = evaluate_model(rfsc, test_data, bootstrap=True)\n",
    "    print(\"Test metrics: \" + str(metrics))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate data\n",
    "import deepsurv\n",
    "\n",
    "def generate_data(treatment_group = False):\n",
    "    np.random.seed(123)\n",
    "    sd = deepsurv.datasets.SimulatedData(5, num_features = 15,\n",
    "        treatment_group = treatment_group)\n",
    "    train_data = sd.generate_data(1000,method = 'linear')\n",
    "    valid_data = sd.generate_data(400,method = 'linear')\n",
    "    test_data = sd.generate_data(400,method = 'linear')\n",
    "\n",
    "    return train_data, valid_data, test_data\n",
    "\n",
    "train_data, valid_data, test_data = generate_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training RSF Model\n",
      "Train/Valid C-Index: 0.775297067435245\n",
      "Valid metrics: {'c_index': 0.747487453310914}\n",
      "Test metrics: {'c_index': 0.7593129571577848, 'c_index_bootstrap': {'mean': 0.7602748345358316, 'confidence_interval': (0.7579763993989844, 0.7625732696726787)}}\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    # Train CPH model\n",
    "    print(\"Training RSF Model\")\n",
    "    train_df = utils.format_dataset_to_df(train_data, 't', 'e')\n",
    "    surv_f = robjects.r(\"as.formula(Surv(t,e) ~ .)\")\n",
    "    rfsc = rfSRC.rfsrc(surv_f, train_df, ntree=100)\n",
    "\n",
    "    print('Train/Valid C-Index:', 1 - rfsc.rx('err.rate')[0][-1])\n",
    "    metrics = evaluate_model(rfsc, valid_data)\n",
    "    print(\"Valid metrics: \" + str(metrics))\n",
    "\n",
    "    metrics = evaluate_model(rfsc, test_data, bootstrap=True)\n",
    "    print(\"Test metrics: \" + str(metrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
